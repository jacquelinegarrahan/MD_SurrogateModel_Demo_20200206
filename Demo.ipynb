{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAKE SURE THAT THE REPOSITORY ROOT IS IN THE PYTHONPATH\n",
    "import sys\n",
    "import os\n",
    "\n",
    "module_path = os.path.abspath(os.path.join(os.pardir, os.pardir))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "MODEL_FILE = \"online_model/files/CNN_051620_SurrogateModel.h5\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling objects\n",
    "Scaling objects are helpful because they allow us to generalize the model architecture to use any scale type preprocessing desired without having to construct a new model with embedded methods. In this case, the scale object is loaded and passed to the model as a parameters. Below is an example of an implementation for the MySurrogateModel example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['JSON', 'backend', 'bins', 'input_names', 'input_offsets', 'input_ordering', 'input_ranges', 'input_scales', 'input_units', 'keras_version', 'layer_names', 'lower', 'ndim', 'output_names', 'output_offsets', 'output_ordering', 'output_scales', 'output_units', 'type', 'upper'])\n"
     ]
    }
   ],
   "source": [
    "from online_model.model.surrogate_model import Scaler, load_model_info\n",
    "model_info = load_model_info(MODEL_FILE)\n",
    "\n",
    "# PRINT INFO CONTAINED IN THE MODEL FILE\n",
    "print(model_info.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the Scaler base class\n",
    "\n",
    "The Scaler class is a base class that requires two methods to be initialized: transform (for input scaling) and inver_transform (for output scaling). This naming scheme is consistent with scikit-learn preprocessing scalers. Otherwise, the implementation is up the the discretion of the user.\n",
    "\n",
    "The separation of the scaler from the model class is helpful because it allows for the separation of model execution logic and because scalers become portable. For instance, if using scikit-learn's MinMaxScaler, you could pickle the object, load the instance you used directly, and plug into the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyScaler(Scaler):\n",
    "    def __init__(self, input_scales, input_offsets, output_scales,\n",
    "                 output_offsets, model_value_min, model_value_max, \n",
    "                 image_input_scales, image_output_scales, \n",
    "                 n_scalar_vars, image_shape):\n",
    "        \n",
    "        self.input_scales = input_scales\n",
    "        self.input_offsets = input_offsets\n",
    "        self.output_scales = output_scales\n",
    "        self.output_offsets = output_offsets\n",
    "        self.model_value_min = model_value_min\n",
    "        self.model_value_max = model_value_max\n",
    "        self.n_scalar_vars = n_scalar_vars\n",
    "        self.image_input_scales = image_input_scales\n",
    "        self.image_output_scales = image_output_scales\n",
    "        self.image_shape = image_shape\n",
    "        \n",
    "    # MUST OVERWRITE\n",
    "    def transform(self, values, scale_type):\n",
    "        if scale_type == \"image\":\n",
    "            data_scaled = values / self.image_input_scale\n",
    "            \n",
    "        elif scale_type == \"scalar\":\n",
    "            data_scaled = self.model_value_min + (\n",
    "                (input_values - self.input_offsets[0 : self.n_scalar_vars])\n",
    "                * (self.model_value_max - self.model_value_min)\n",
    "                / self.input_scales)\n",
    "        \n",
    "        return data_scaled\n",
    "    \n",
    "    # MUST OVERWRITE\n",
    "    def inverse_transform(self, values, scale_type):\n",
    "        if scale_type == \"image\":\n",
    "            data_unscaled = image_values * self.image_output_scale\n",
    "        \n",
    "        elif scale_type == \"scalar\":\n",
    "            # reshape values\n",
    "            \n",
    "            values = values.reshape(values.shape[0], self.image_shape)    \n",
    "    \n",
    "            data_unscaled = (\n",
    "                (input_values - self.min_value)\n",
    "                * (self.input_scales[: self.n_scalar_vars])\n",
    "                / (self.model_value_max - self.model_value_min)\n",
    "            ) + self.input_offsets[: self.n_scalar_vars]\n",
    "        \n",
    "            data_unscaled = data_unscaled.reshape(self.image_shape)\n",
    "            \n",
    "        return data_unscaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MyScalar should be initialized with model info provided with the file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get model info from file and initialize appropriately\n",
    "model_info = load_model_info(MODEL_FILE)\n",
    "\n",
    "# prepare info necessary to initialize\n",
    "image_input_scales = model_info[\"input_scales\"][-1]\n",
    "image_output_scales = model_info[\"output_scales\"][-1]\n",
    "image_offset = model_info[\"output_offsets\"][-1]\n",
    "output_scales = model_info[\"output_scales\"][:-1]\n",
    "output_offsets = model_info[\"output_offsets\"][:-1]\n",
    "n_scalar_vars = len(model_info[\"input_ordering\"])\n",
    "n_scalar_outputs = len(model_info[\"output_ordering\"])\n",
    "input_scales = model_info[\"input_scales\"][: n_scalar_vars]\n",
    "input_offsets = model_info[\"input_offsets\"][: n_scalar_vars]\n",
    "model_value_min = model_info[\"lower\"]\n",
    "model_value_max = model_info[\"upper\"]\n",
    "image_shape = (model_info[\"bins\"][0], model_info[\"bins\"][1])\n",
    "\n",
    "# Create instance of scaler object\n",
    "my_scaler_obj = MyScaler(input_scales, input_offsets, output_scales,\n",
    "                 output_offsets, model_value_min, model_value_max, \n",
    "                 image_input_scales, image_output_scales, \n",
    "                 n_scalar_vars, image_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE A NEW MODEL CLASS\n",
    "# import base classes\n",
    "from online_model.model.surrogate_model import (SurrogateModel, \n",
    "                                                ReconstructedScaler,\n",
    "                                                apply_temporary_ordering_patch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the SurrogateModel base class\n",
    "\n",
    "The SurrogateModel class is the base class used for creating the server/model interface. The session and graph set up in the base class are necessary for running the model in a thread-safe manner. \n",
    "\n",
    "SurrogateModel provides an abstract method: `predict`. This means that the `predict` function MUST be implemented by the any class that inherits from this class. This is important because it allows us to generalize the server callbacks to call `predict` against any model implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySurrogateModel(SurrogateModel):\n",
    "    \n",
    "    def __init__(self, model_file, scaler, stock_image_input):\n",
    "        # Below line does all the work in loading + setting up \n",
    "        # the model session by calling the __init__ method from\n",
    "        # the SurrogateModel class\n",
    "        super(MySurrogateModel, self).__init__(model_file)\n",
    "        self.model_file = model_file\n",
    "        self.scaler = scaler\n",
    "        self.stock_image_input = stock_image_input\n",
    "        \n",
    "        \n",
    "        # can use the utility function to load the model info\n",
    "        # and populate needed info\n",
    "        model_info = load_model_info(model_file)\n",
    "        self.ndim = model_info[\"ndim\"]\n",
    "        self.output_ordering = len(model_info[\"output_ordering\"])\n",
    "        \n",
    "        \n",
    "        # TEMPORARY PATCH FOR INPUT/OUTPUT REDUNDANT VARS\n",
    "        self.input_ordering = apply_temporary_ordering_patch(\n",
    "            self.input_ordering, \"in\")\n",
    "        self.output_ordering = apply_temporary_ordering_patch(\n",
    "            self.input_ordering, \"out\")\n",
    "    \n",
    "    # no need for an evaluate method, \n",
    "    # can handle everything in predict\n",
    "    def predict(self, input_values):\n",
    "        \n",
    "        # scale inputs\n",
    "        image_input = self.scaler.transform(np.array(input_values[\"image\"]),\n",
    "                                            \"image\")\n",
    "        other_inputs = np.array([settings[key] for key in \n",
    "                                self.input_ordering])\n",
    "        other_inputs = self.scaler.transform(other_inputs, \"scalar\")\n",
    "        \n",
    "        # now that this is scaled, we access the model attribute loaded by the \n",
    "        # SurrogateModel base class\n",
    "        predicted_output = self.model.predict(\n",
    "            [inputs_image_scaled, inputs_scalar_scaled]\n",
    "        )\n",
    "        \n",
    "        # process the model output\n",
    "        image_output = np.array(predicted_output[0])\n",
    "        scalar_output = predicted_output[1]\n",
    "        \n",
    "        \n",
    "        # unscale \n",
    "        image_output = self.scaler.inverse_transform(image_output, \"image\")\n",
    "        scalar_output = self.scaler.inverse_transform(scalar_output, \"scalar\")\n",
    "        \n",
    "        # select extents\n",
    "        extent_output = scalar_output[\n",
    "            :, int(len(self.output_ordering) - self.ndim[0]) :\n",
    "        ]\n",
    "        \n",
    "        # Now, format for server use\n",
    "        formatted_output = dict(zip(self.output_ordering, scalar_output.T))\n",
    "        formatted_output[\"extents\"] = extent_output\n",
    "        formatted_output[\"image\"] = image_output\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "online-surrogate-model-dev",
   "language": "python",
   "name": "online-surrogate-model-dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
